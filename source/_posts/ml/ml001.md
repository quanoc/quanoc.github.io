---
layout:     post
title:      "简单的神经网络"
subtitle:   "深度学习"
date:       2019-05-26 15:00:00
catalog:    true
author:     "Nova"
header-img: "/img/post-img/deepLearning.jpg"
tags:
    - Deep Learning
---



### 感知器分类
Perceptron

算法步骤

权重向量W,训练样本X

1. 把权重向量初始化为0，或把每个分量初始化为[0,1]间任意小数
2. 把训练样本输入感知器，得到分类结果（-1或1）
3. 根据分类结果更新权重向量

神经元会进行以下算法步骤：

```
将神经元接受到的信号x进行整合：
z=w1x1+⋯+wmxm

用激活函数对z进行处理，例如下面的步调函数：
ϕ(z)={1−1if z≥θif > otherwise
```

#### 激活函数

激活函数是用来加入非线性因素的，因为线性模型的表达能力不够。

所谓激活函数（Activation Function），就是在人工神经网络的神经元上运行的函数，负责将神经元的输入映射到输出端。

激活函数（Activation functions）对于人工神经网络 [1]  模型去学习、理解非常复杂和非线性的函数来说具有十分重要的作用。它们将非线性特性引入到我们的网络中

在神经元中，输入的 inputs 通过加权，求和后，还被作用了一个函数，这个函数就是激活函数。引入激活函数是为了增
加神经网络模型的非线性。没有激活函数的每层都相当于矩阵相乘。就算你叠加了若干层之后，无非还是个矩阵相乘罢了。

#### 为什么要用激活函数

如果不用激活函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合，这种情况就是最原始的感知机（Perceptron）。

如果使用的话，激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中。

激活函数的主要作用是提供网络的非线性建模能力。如果没有激活函数，那么该网络仅能够表达线性映射，此时即便有再多的隐藏层，其整个网络跟单层神经网络也是等价的。因此也可以认为，只有加入了激活函数之后，深度神经网络才具备了分层的非线性映射学习能力。 

#### 激活函数特性 
可微性： 当优化方法是基于梯度的时候，这个性质是必须的。 
单调性： 当激活函数是单调的时候，单层网络能够保证是凸函数。 
输出值的范围： 当激活函数输出值是 有限 的时候，基于梯度的优化方法会更加 稳定，因为特征的表示受有限权值的影响更显著;当激活函数的输出是 无限 的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的learning rate 

从目前来看，常见的激活函数多是分段线性和具有指数形状的非线性函数 


#### 常用的激活函数
Sigmoid函数：Sigmoid函数是一个在生物学中常见的S型函数，也称为S型生长曲线。在信息科学中，由于其单增以及反函数单增等性质，Sigmoid函数常被用作神经网络的阈值函数，将变量映射到0,1之间。

Tanh函数：Tanh是双曲函数中的一个，Tanh()为双曲正切。在数学中，双曲正切“Tanh”是由基本双曲函数双曲正弦和双曲余弦推导而来。

ReLU函数：Relu激活函数（The Rectified Linear Unit），用于隐层神经元输出。

[26种神经网络激活函数可视化](https://www.jiqizhixin.com/articles/2017-10-10-3)



### 步调函数与阈值。
```
w0 = -θ(w0为阈值) and x0 = 1
z = w0x0+w1x1+...+wmxm=wTx and Φ(z) = 1 , if z ≥ θ; -1, otherwise;
```

### 权重更新算法
```
wj=wj+∇wj
∇wj=η∗(y−y′)∗xj
η 表示学习率，是一个 [0, 1] 之间的小数；
y 是输入样本的正确分类，y’ 是感知器计算出来的分类。
```

学习率太高，会快速拟合。学习率太慢，耗时太长。

### 阈值的更新
阈值并不是一成不变的，初次的阈值可以人为制定，以后会不断的自动更新。 
阈值更新如下：

```
w0=0，∇w0=η∗(y−y′)
y=1，y′=−1
w0=w0+∇w0=0+0.3∗(1−(−1))=0.6
```

### 感知器算法适用范围
数据分布线性可分割。


### 自适用性神经元
激活函数不再是步调函数，

渐进下降

和方差求导数