---
layout:     post
title:      "FastText原理探索"
subtitle:   "What is fastText"
date:       2018-08-18
catalog:    true
author:     "Nova"
header-img: "/img/post-img/0003.jpg"
tags:
    - fasttext
    - NLP
---

Facebook于2016年开源的一个**词向量计算**和**文本分类**工具。
它的有点非常明显，在文本分类任务中,fasttext(浅层网络)往往能取得和深度网络相媲美的精度，却在训练时间上比深度网络快许多数量级。

### 预备知识
softmax、ngram、word2vec原理

之后在介绍fasttext的原理、使用keras搭建简单的分类器

#### 1. softmax
softmax回归又被称作多项逻辑回归，它是逻辑回归在处理多类别任务上的推广。

在逻辑回归中，有m个被标注的样本，因为类标时二元的，所以y的值域（0,1）.

在softmax回归中，类标是大于2的，因此对于训练集，y的值域(1,2,...,K).

给定一个测试输入x,我们假设输出一个K维的向量，向量内每个元素的值表示x属于当前类别的概率。

分层softmax: 在k值较大时，标准的softmax回归中，要计算y=j时的softmax概率:P(y=j),需要对所有K个概率做归一化，y值越大越耗时。
于是，有了分层softmax.基本思想是使用树的层次结构替代扁平化的标准softmax.只需要计算一条路径上的所有节点的概率值。

通过分层的softmax,计算复杂度一下从K降到了logK.

#### 2. n-gram特征
在文本特征提取中，常常看到n-gram的身影。它是一种基于*语言模型*的算法。基本思想是将文本内容按照字节顺序进行大小为N的滑动窗口操作，最终形成长度为N的字节片段序列。

举例： 我来到达观数据参观

相应的bigram特征为：我来 来到 到达 达观 观数 数据 据参 参观
相应的trigram特征为：我来到 来到达 到达观 达观数 观数据 数据参 据参观

> n-gram 中的gram根据粒度不同，有不同的含义，它可以时字粒度，也可以是词粒度的。上面的例子都是字粒度的n-gram,词粒度的n-gram看下面例子。



举例：我 来到 达观数据 参观

相应的bigram特征为：我/来到 来到/达观数据 达观数据/参观
相应的trigram特征为：我/来到/达观数据 来到/达观数据/参观


n-gram产生的特征只是作为文本特征的候选集，你后面可能会采用信息熵、卡方统计、IDF等文本特征选择方式筛选出比较重要特征。

### Word2vec
为什么介绍word2vec.主要的原因时word2vec的CBOM模型架构和fastText模型非常相似。

fasttext不仅仅是文本分类工具，还实现了快速词向量训练工具。

> word2vec主要有两种模型：skip-gram模型和CBOW模型

#### 1. CBOW模型
基本思路是：用上下文预测目标词汇。

输入层由目标词汇y的上下文单词{x1,...,xc}组成，xi是被onehot编码过的V维向量，其中V是词汇量；隐含层是N维向量h；
输出层是被onehot编码过的目标词y。
输入向量通过 V*N维的权重矩阵W链接到隐含层；隐含层通过N*V维的权重矩阵W连接到输出层。因为词库V往往非常大，使用标准的softmax计算相当耗时，于是CBOW的输出层采用的正是上文提到过的分层Softmax。


### fasttext
仔细观察模型的后半部分，即从隐含层输出到输出层输出，会发现它就是一个softmax线性多类别分类器，分类器的输入是一个用来表征当前文档的向量；模型的前半部分，即从输入层输入到隐含层输出部分，主要在做一件事情：生成用来表征文档的向量。那么它是如何做的呢？叠加构成这篇文档的所有词及n-gram的词向量，然后取平均。叠加词向量背后的思想就是传统的词袋法，即将文档看成一个由词构成的集合。


于是fastText的核心思想就是：将整篇文档的词及n-gram向量叠加平均得到文档向量，然后使用文档向量做softmax多分类。这中间涉及到两个技巧：字符级n-gram特征的引入以及分层Softmax分类。

#### 关于效果
```
我 来到 达观数据
俺 去了 达而观信息科技
```

这两段文本意思几乎一模一样，如果要分类，肯定要分到同一个类中去。但在传统的分类器中，用来表征这两段文本的向量可能差距非常大。传统的文本分类中，你需要计算出每个词的权重，比如tfidf值， “我”和“俺” 算出的tfidf值相差可能会比较大，其它词类似，于是，VSM（向量空间模型）中用来表征这两段文本的文本向量差别可能比较大。


但是fastText就不一样了，它是用单词的embedding叠加获得的文档向量，词向量的重要特点就是向量的距离可以用来衡量单词间的语义相似程度，于是，在fastText模型中，这两段文本的向量应该是非常相似的，于是，它们很大概率会被分到同一个类中。

[http://www.52nlp.cn/fasttext](http://www.52nlp.cn/fasttext)



#### TODO
字符级n-gram特征、使用词embedding而并非词本身作为特征。
CBOM模型架构机器应用\word2vec


